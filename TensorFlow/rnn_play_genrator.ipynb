{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.utils import pad_sequences\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))\n",
    "char2idx = {u:i for i,u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "def text_to_int(text):\n",
    "    return np.array([char2idx[c] for c in text])\n",
    "\n",
    "text_as_int = text_to_int(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_text(ints):\n",
    "    try:\n",
    "        ints = ints.numpy()\n",
    "    except:\n",
    "        pass\n",
    "    return ''.join(idx2char[ints])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(18, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100 #Hell -> ello ==> Hello(input+output)\n",
    "examples_per_epoch = len(text)//(seq_length+1) #학습할때 101개의 단어들이 필요하기에 epoch를 이렇게 설정한다\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int) #array를 잘라서 \n",
    "print(list(char_dataset)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    output_text = chunk[1:]\n",
    "    return input_text, output_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE = len(vocab)\n",
    "EMBEDDING_DIM = 256\n",
    "RNN_UNITS = 1024\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "#shuffle을 하는 이유는 단어 학습을 시키기 위함이고 나중에 문장 학습 할때는 batch size = 1, no shuffle로 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (64, None, 256)           16640     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (64, None, 1024)          5246976   \n",
      "                                                                 \n",
      " dense_2 (Dense)             (64, None, 65)            66625     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                                batch_input_shape=[batch_size, None]), #None으로 하는 이유는 나중에 이 모델을 쓸때 얼마나 긴 문장을 쓸지 모르기때문\n",
    "        tf.keras.layers.LSTM(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in data.take(1):\n",
    "    example_batch_prediction = model(input_example_batch)\n",
    "    print(example_batch_prediction.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "tf.Tensor(\n",
      "[[[-1.78233965e-03  1.50419958e-03 -3.88172455e-04 ...  3.15168081e-03\n",
      "   -6.03858987e-03  2.92667607e-03]\n",
      "  [-1.03026943e-03 -8.37136758e-05  3.00721498e-04 ... -3.71807930e-03\n",
      "   -2.27467157e-03 -1.72761700e-03]\n",
      "  [-3.64894513e-03 -2.04798672e-03  6.75425772e-03 ... -2.57171644e-03\n",
      "   -4.10743244e-03  9.11758339e-04]\n",
      "  ...\n",
      "  [-6.27422472e-03 -3.83061310e-03  5.94043639e-03 ... -1.54265091e-02\n",
      "   -1.17436759e-02 -3.76639003e-03]\n",
      "  [-6.37861015e-03 -4.26725578e-03  3.98376957e-03 ... -1.12548089e-02\n",
      "   -1.43368207e-02 -5.46216127e-03]\n",
      "  [-5.31135732e-03  5.80312964e-03 -2.46041454e-05 ... -8.52638204e-03\n",
      "   -1.52471503e-02 -4.28096205e-03]]\n",
      "\n",
      " [[ 1.53944886e-04 -7.89211597e-04  1.26124639e-03 ... -6.13791170e-03\n",
      "    2.89706048e-03 -3.71359335e-03]\n",
      "  [-2.72703427e-03 -1.41478702e-03  3.38712684e-03 ... -6.64508017e-03\n",
      "    7.77630601e-04 -6.99822977e-03]\n",
      "  [-4.55133291e-03  2.61627021e-03  7.93362502e-03 ... -4.97110514e-03\n",
      "    1.27438060e-03 -1.91380095e-03]\n",
      "  ...\n",
      "  [-2.76013045e-03  6.11715252e-03  1.30721321e-03 ... -7.75501225e-03\n",
      "    9.46058892e-04 -3.75909149e-04]\n",
      "  [-9.26198065e-03  8.83861445e-03  6.93600439e-03 ... -5.08430367e-03\n",
      "    3.78177408e-03 -3.67396325e-03]\n",
      "  [-7.34290294e-03  1.45823555e-02  2.48075859e-03 ... -4.15380392e-03\n",
      "   -5.21329697e-04 -2.08133063e-03]]\n",
      "\n",
      " [[ 2.51567480e-03 -2.52283807e-03  1.93273905e-03 ... -4.77428036e-03\n",
      "   -5.76300360e-03  2.37789005e-03]\n",
      "  [ 4.90017468e-04  5.71543910e-03 -1.67184148e-03 ... -2.70655425e-03\n",
      "   -8.70194845e-03  2.48515164e-03]\n",
      "  [-4.56407573e-03  2.16244394e-03  4.44736285e-03 ... -1.11592840e-03\n",
      "   -7.41526671e-03  4.09779744e-03]\n",
      "  ...\n",
      "  [-6.71386020e-03  1.61315575e-02 -8.21063574e-03 ... -4.41129738e-03\n",
      "   -1.72004607e-02 -2.07769545e-03]\n",
      "  [-4.00333013e-03  1.21500408e-02 -1.09542999e-02 ... -5.98700671e-03\n",
      "   -1.61660165e-02 -6.65741973e-04]\n",
      "  [-5.22159785e-03  1.13103259e-02 -8.92727543e-03 ... -1.80306006e-03\n",
      "   -2.00726315e-02  1.79857202e-03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-2.05285061e-04  7.52405636e-03 -2.70678313e-03 ...  2.03434378e-04\n",
      "   -4.79433639e-03  2.44431314e-04]\n",
      "  [-1.35266851e-03 -4.62324795e-04  6.89546485e-03 ...  3.03975772e-03\n",
      "   -1.63661630e-03 -4.25341399e-03]\n",
      "  [ 1.00886461e-03 -3.18186567e-03  7.03070872e-03 ... -3.28827905e-03\n",
      "   -6.78582257e-03 -4.37595532e-04]\n",
      "  ...\n",
      "  [ 1.01898040e-03 -1.16638979e-03  9.33893025e-03 ... -1.18434466e-02\n",
      "    8.58640647e-04 -4.98169614e-03]\n",
      "  [-8.22467497e-04 -3.78555153e-03  4.49387403e-03 ... -2.26040441e-03\n",
      "   -1.10607105e-03 -2.20683706e-03]\n",
      "  [-1.94378581e-03  4.73585306e-03  3.74457450e-04 ... -1.85325276e-03\n",
      "   -4.97171190e-03 -5.05970092e-04]]\n",
      "\n",
      " [[-6.42613368e-03 -3.30457959e-04 -2.65508192e-03 ...  1.70138781e-03\n",
      "   -1.66628254e-03  1.76207349e-03]\n",
      "  [-6.68046949e-03 -6.03271229e-03  6.86581433e-03 ...  4.41308366e-03\n",
      "    7.77829206e-04 -3.34908348e-03]\n",
      "  [-6.18982175e-03  2.75086705e-03  2.42031319e-03 ...  2.83490214e-03\n",
      "   -3.96291865e-03 -2.07687705e-03]\n",
      "  ...\n",
      "  [-4.23514331e-03  5.57229016e-03  4.99249902e-04 ...  4.78918105e-03\n",
      "   -6.04667934e-03 -3.28448694e-03]\n",
      "  [-6.33927109e-03  4.36867122e-03  2.15480570e-03 ...  2.59291986e-03\n",
      "   -6.17072312e-03 -7.94424862e-03]\n",
      "  [ 3.09695397e-03  3.03402962e-03  3.68821295e-03 ...  2.44644401e-03\n",
      "    3.24554066e-03 -7.28063937e-03]]\n",
      "\n",
      " [[-6.66914647e-03  1.81649439e-03 -1.12240994e-03 ... -3.57589778e-03\n",
      "    1.43253768e-03 -2.16267910e-03]\n",
      "  [-1.21677546e-02  3.85953649e-03 -3.46856099e-03 ... -5.18441107e-03\n",
      "   -2.61892984e-03  2.85770465e-03]\n",
      "  [-2.21356843e-03  3.00155836e-04  2.30146502e-03 ...  1.34787022e-03\n",
      "    2.24925857e-03  3.03479331e-03]\n",
      "  ...\n",
      "  [-3.37907858e-03  2.75947573e-03  3.38863162e-03 ... -1.02293119e-02\n",
      "   -1.26253897e-02  4.32053988e-04]\n",
      "  [-7.83166266e-04  2.02046777e-03  3.20986821e-03 ... -1.32463891e-02\n",
      "   -7.71069806e-03 -3.67918494e-03]\n",
      "  [ 9.86157544e-03  3.01231211e-03 -1.92182628e-03 ... -1.38963498e-02\n",
      "   -4.17319033e-03 -1.73962908e-04]]], shape=(64, 100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#배치 갯수\n",
    "print(len(example_batch_prediction))\n",
    "print(example_batch_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "tf.Tensor(\n",
      "[[-1.7823396e-03  1.5041996e-03 -3.8817246e-04 ...  3.1516808e-03\n",
      "  -6.0385899e-03  2.9266761e-03]\n",
      " [-1.0302694e-03 -8.3713676e-05  3.0072150e-04 ... -3.7180793e-03\n",
      "  -2.2746716e-03 -1.7276170e-03]\n",
      " [-3.6489451e-03 -2.0479867e-03  6.7542577e-03 ... -2.5717164e-03\n",
      "  -4.1074324e-03  9.1175834e-04]\n",
      " ...\n",
      " [-6.2742247e-03 -3.8306131e-03  5.9404364e-03 ... -1.5426509e-02\n",
      "  -1.1743676e-02 -3.7663900e-03]\n",
      " [-6.3786102e-03 -4.2672558e-03  3.9837696e-03 ... -1.1254809e-02\n",
      "  -1.4336821e-02 -5.4621613e-03]\n",
      " [-5.3113573e-03  5.8031296e-03 -2.4604145e-05 ... -8.5263820e-03\n",
      "  -1.5247150e-02 -4.2809620e-03]], shape=(100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#100개의 time step 예측들\n",
    "pred = example_batch_prediction[0]\n",
    "print(len(pred))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "tf.Tensor(\n",
      "[-1.7823396e-03  1.5041996e-03 -3.8817246e-04 -3.9265936e-04\n",
      "  3.1650728e-03 -4.0468574e-03 -1.7952236e-03  2.4061592e-04\n",
      " -1.9356509e-03 -1.1999721e-03 -9.2524663e-04  8.0917799e-04\n",
      "  2.6031784e-03 -8.2836353e-04  3.1787497e-03  5.3058388e-03\n",
      " -3.4365058e-03  1.8296164e-03  5.9658079e-05 -2.4150172e-04\n",
      "  5.1288912e-03 -1.0444942e-03  6.4438297e-03 -2.3077161e-03\n",
      " -2.5550909e-03  2.7100041e-03  7.8152516e-04 -2.5708636e-03\n",
      " -3.7814374e-04 -4.7471980e-03 -1.7017433e-03  7.2944211e-04\n",
      " -4.5172847e-04  2.3371023e-03  7.4250055e-03 -2.2213510e-03\n",
      " -2.5738417e-03 -4.9755317e-03 -2.1055851e-03 -1.3521982e-03\n",
      " -3.8657421e-03  3.0829967e-03  3.6945543e-03  2.2579385e-03\n",
      " -3.4161941e-03  1.0992354e-04  2.4486880e-04 -4.3441178e-03\n",
      "  1.5874807e-03 -4.4112979e-03  3.6623906e-03  4.0790723e-03\n",
      "  4.1058445e-03 -1.6618724e-04 -3.6860721e-03  4.5791303e-04\n",
      "  3.7779808e-03  2.1755155e-03  5.1029515e-04  1.7979462e-04\n",
      "  4.8551438e-03 -2.0518475e-03  3.1516808e-03 -6.0385899e-03\n",
      "  2.9266761e-03], shape=(65,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#65개의 글자 예측\n",
    "time_step = pred[0]\n",
    "print(len(time_step))\n",
    "print(time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mYUXTUc;uvW cyMMlaViluUQs.$on!dpPPtH GRCma!\\nvxSVqwnCUedG:?dJmOFd?-JK&ycNzdlY\\neTpZdrANZNrRwza&BDXTppJ'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_indices = tf.random.categorical(pred, num_samples=1)\n",
    "\n",
    "sample_indices = np.reshape(sample_indices, (1, -1))[0]\n",
    "predicted_chars = int_to_text(sample_indices)\n",
    "\n",
    "predicted_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits): #logits = probability distribution\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(data, epochs=40, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(tf.train.lastest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    #만들 단어 갯수\n",
    "    num_generate = 800\n",
    "\n",
    "    #시작 단어를 모델의 인풋에 맞게 조절\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0) #[1,2,3] -> [[1,2,3]]\n",
    "\n",
    "    text_generated = []\n",
    "    temperature = 1.0 # 낮을수록 모델에서 구한 posibility distribution을 따르고 높을수록 안따름\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(prediction, 0) #[[1,2,3]] -> [1,2,3]\n",
    "\n",
    "        #categorical distribution 작업\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        input_eval = tf.expand_dims(idx2char[predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = input(\"input starting string: \")\n",
    "print(generated_text(inp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
