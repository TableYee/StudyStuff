{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.utils import pad_sequences\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))\n",
    "char2idx = {u:i for i,u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "def text_to_int(text):\n",
    "    return np.array([char2idx[c] for c in text])\n",
    "\n",
    "text_as_int = text_to_int(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_text(ints):\n",
    "    try:\n",
    "        ints = ints.numpy()\n",
    "    except:\n",
    "        pass\n",
    "    return ''.join(idx2char[ints])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-28 20:44:47.125029: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(18, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100 #Hell -> ello ==> Hello(input+output)\n",
    "examples_per_epoch = len(text)//(seq_length+1) #학습할때 101개의 단어들이 필요하기에 epoch를 이렇게 설정한다\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int) #array를 잘라서 \n",
    "print(list(char_dataset)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    output_text = chunk[1:]\n",
    "    return input_text, output_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE = len(vocab)\n",
    "EMBEDDING_DIM = 256\n",
    "RNN_UNITS = 1024\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "#shuffle을 하는 이유는 단어 학습을 시키기 위함이고 나중에 문장 학습 할때는 batch size = 1, no shuffle로 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (64, None, 256)           16640     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
      "                                                                 \n",
      " dense (Dense)               (64, None, 65)            66625     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                                batch_input_shape=[batch_size, None]), #None으로 하는 이유는 나중에 이 모델을 쓸때 얼마나 긴 문장을 쓸지 모르기때문\n",
    "        tf.keras.layers.LSTM(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in data.take(1):\n",
    "    example_batch_prediction = model(input_example_batch)\n",
    "    print(example_batch_prediction.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'example_batch_prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/euncheolj/Downloads/StudyStuff/TensorFlow/rnn_play_genrator.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/euncheolj/Downloads/StudyStuff/TensorFlow/rnn_play_genrator.ipynb#ch0000011?line=0'>1</a>\u001b[0m \u001b[39m#배치 갯수\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/euncheolj/Downloads/StudyStuff/TensorFlow/rnn_play_genrator.ipynb#ch0000011?line=1'>2</a>\u001b[0m data\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/euncheolj/Downloads/StudyStuff/TensorFlow/rnn_play_genrator.ipynb#ch0000011?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(example_batch_prediction))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/euncheolj/Downloads/StudyStuff/TensorFlow/rnn_play_genrator.ipynb#ch0000011?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(example_batch_prediction)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'example_batch_prediction' is not defined"
     ]
    }
   ],
   "source": [
    "#배치 갯수\n",
    "print(len(example_batch_prediction))\n",
    "print(example_batch_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "tf.Tensor(\n",
      "[[-1.7823396e-03  1.5041996e-03 -3.8817246e-04 ...  3.1516808e-03\n",
      "  -6.0385899e-03  2.9266761e-03]\n",
      " [-1.0302694e-03 -8.3713676e-05  3.0072150e-04 ... -3.7180793e-03\n",
      "  -2.2746716e-03 -1.7276170e-03]\n",
      " [-3.6489451e-03 -2.0479867e-03  6.7542577e-03 ... -2.5717164e-03\n",
      "  -4.1074324e-03  9.1175834e-04]\n",
      " ...\n",
      " [-6.2742247e-03 -3.8306131e-03  5.9404364e-03 ... -1.5426509e-02\n",
      "  -1.1743676e-02 -3.7663900e-03]\n",
      " [-6.3786102e-03 -4.2672558e-03  3.9837696e-03 ... -1.1254809e-02\n",
      "  -1.4336821e-02 -5.4621613e-03]\n",
      " [-5.3113573e-03  5.8031296e-03 -2.4604145e-05 ... -8.5263820e-03\n",
      "  -1.5247150e-02 -4.2809620e-03]], shape=(100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#100개의 time step 예측들\n",
    "pred = example_batch_prediction[0]\n",
    "print(len(pred))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "tf.Tensor(\n",
      "[-1.7823396e-03  1.5041996e-03 -3.8817246e-04 -3.9265936e-04\n",
      "  3.1650728e-03 -4.0468574e-03 -1.7952236e-03  2.4061592e-04\n",
      " -1.9356509e-03 -1.1999721e-03 -9.2524663e-04  8.0917799e-04\n",
      "  2.6031784e-03 -8.2836353e-04  3.1787497e-03  5.3058388e-03\n",
      " -3.4365058e-03  1.8296164e-03  5.9658079e-05 -2.4150172e-04\n",
      "  5.1288912e-03 -1.0444942e-03  6.4438297e-03 -2.3077161e-03\n",
      " -2.5550909e-03  2.7100041e-03  7.8152516e-04 -2.5708636e-03\n",
      " -3.7814374e-04 -4.7471980e-03 -1.7017433e-03  7.2944211e-04\n",
      " -4.5172847e-04  2.3371023e-03  7.4250055e-03 -2.2213510e-03\n",
      " -2.5738417e-03 -4.9755317e-03 -2.1055851e-03 -1.3521982e-03\n",
      " -3.8657421e-03  3.0829967e-03  3.6945543e-03  2.2579385e-03\n",
      " -3.4161941e-03  1.0992354e-04  2.4486880e-04 -4.3441178e-03\n",
      "  1.5874807e-03 -4.4112979e-03  3.6623906e-03  4.0790723e-03\n",
      "  4.1058445e-03 -1.6618724e-04 -3.6860721e-03  4.5791303e-04\n",
      "  3.7779808e-03  2.1755155e-03  5.1029515e-04  1.7979462e-04\n",
      "  4.8551438e-03 -2.0518475e-03  3.1516808e-03 -6.0385899e-03\n",
      "  2.9266761e-03], shape=(65,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#65개의 글자 예측\n",
    "time_step = pred[0]\n",
    "print(len(time_step))\n",
    "print(time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mYUXTUc;uvW cyMMlaViluUQs.$on!dpPPtH GRCma!\\nvxSVqwnCUedG:?dJmOFd?-JK&ycNzdlY\\neTpZdrANZNrRwza&BDXTppJ'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_indices = tf.random.categorical(pred, num_samples=1)\n",
    "\n",
    "sample_indices = np.reshape(sample_indices, (1, -1))[0]\n",
    "predicted_chars = int_to_text(sample_indices)\n",
    "\n",
    "predicted_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits): #logits = probability distribution\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(data, epochs=40, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(tf.train.lastest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    #만들 단어 갯수\n",
    "    num_generate = 800\n",
    "\n",
    "    #시작 단어를 모델의 인풋에 맞게 조절\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0) #[1,2,3] -> [[1,2,3]]\n",
    "\n",
    "    text_generated = []\n",
    "    temperature = 1.0 # 낮을수록 모델에서 구한 posibility distribution을 따르고 높을수록 안따름\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(prediction, 0) #[[1,2,3]] -> [1,2,3]\n",
    "\n",
    "        #categorical distribution 작업\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        input_eval = tf.expand_dims(idx2char[predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = input(\"input starting string: \")\n",
    "print(generated_text(inp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
